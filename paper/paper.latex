%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf, screen=true]{acmart}
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn


\usepackage{booktabs} % For formal tables
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{import}
\usepackage{hyperref}
\subimport{layers/}{init}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
% setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
% \editor{Derrick Amponsa Afrifa}
% \editor{Luke Matthew Lau}
% \editor{Martino Mansoldo}


\begin{document}
% We need a short and succinct title
\title{CycleGAN Two Ways}
\subtitle{A Comparison of Unpaired Image-to-Image Translation Performance with Cycle-Consistent Adversarial Networks implemented with different Machine Learning frameworks}


\author{Derrick Amponsa Afrifa}
\affiliation{
  \institution{Trinity College, University of Dublin}
  \city{Dublin}
  \state{Republic of Ireland}
}
\email{afrifad@tcd.ie}

\author{Luke Lau}
\affiliation{
  \institution{Trinity College, University of Dublin}
  \city{Dublin}
  \state{Republic of Ireland}
}
\email{laulu@tcd.ie}

\author{Martino Mansoldo}
\affiliation{
  \institution{Trinity College, University of Dublin}
  \city{Dublin}
  \country{Republic of Ireland}
 }
\email{mansoldm@tcd.ie}

\begin{abstract}
Although many Machine Learning libraries and frameworks implement the same set of algorithms, the manner in which they are implemented differs from library to library. One rarely finds objectively superior implementations. 
% However, a number of key metrics can be used to evaluate the strengths and shortcomings of dissimilar approaches. 
This paper attempts to outline the differences that result from implementations of CycleGAN in PyTorch and TensorFlow, two popular frameworks for Machine Learning. Models are trained using the same dataset and key aspects of their outputs are delineated. It is expected that an understanding of the nature of the differences will reveal new insights.

% \break
% 1.Introduction. In one sentence, what's the topic?\break
% 2.State the problem you tackle\break
% 3.Summarize (in one sentence) why nobody else has adequately answered the research question yet\break
% 4.Explain, in one sentence, how you tackled the research question\break
% 5.In one sentence, how did you go about doing the research that follows from your big idea.\break
% 6.As a single sentence, what's the key impact of your research
\end{abstract}

\keywords{machine learning, neural networks, generative adversarial networks, tensorflow, pytorch}


\maketitle

\section{Introduction}
Generative Adversarial Networks (GANs)\cite{gan} are a type of neural network, based around the architecture of a generator and discriminator competing against each other.
The generator tries to generate images from noise that are similar to the training data, whilst the discriminator tries to catch out the generator and distinguish the "fake" generated images from the real images.
CycleGAN builds upon this by having two generators that translate images between two categories, $G: X \rightarrow Y$ and $F: Y \rightarrow X$. Additionally, it adds a cycle-consistency loss, where translated images are converted back to their original category and the difference between original and reconstructed images is minimised, so that $F(G(X)) \approx X$ holds.

TensorFlow and PyTorch have either APIs built into them or faithful implementations of the model available for CycleGAN. We aim to find what  differences exist when implementing identical CycleGAN model architectures in these frameworks. 

\section{Related Work}
PyTorch began as the Python implementation of Torch\cite{torch}, a Machine Learning framework for Lua, and provides a large amount of extensibility for implementing deep architectures\cite{bahrampour2015comparative}. It builds up a computation graph dynamically as the program is run.
In contrast, TensorFlow\cite{abadi2016tensorflow}, developed at Google, builds up its computation graph before training and evaluating. This graph describes the flow of tensors throughout the network, hence the etymology.
Benchmarks\cite{shi2016benchmarking} of Torch and TensorFlow have shown that neither framework has a particular advantage in training time on GPUs.  

\section{Methodology}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \pic at (0,0,0) {ConvRelu={blockname=conv1,
            numFilters={{"3",""}},height=10,width={0.5},depth=10,zlabel=256}};
        \pic[shift={(0.5,0,0)}] at (conv1-east) {ConvRelu={blockname=conv2,
            numFilters={{"64",""}},height=10,width=2,depth=10,zlabel=256}};
        \pic[shift={(0.5,0,0)}] at (conv2-east) {ConvRelu={blockname=conv3,
            numFilters={{"128",""}},height=5,width=4,depth=5,zlabel=128}};
        \pic[shift={(0.5,0,0)}] at (conv3-east) {GenericSequentialLayer={blockname=resnet,
            numFilters={{"","","","","$64\times9$","","","",""}},height=2.5,width={1,1,1,1,1,1,1,1,1},depth=2.5,zlabel=64}};
        \pic[shift={(0,0,0)}] at (resnet-east) {Deconv={blockname=deconv1,
            numFilters={{"128",""}},height=5,width=4,depth=5,zlabel=128}};
        \pic[shift={(0.5,0,0)}] at (deconv1-east) {Deconv={blockname=deconv2,
            numFilters={{"64",""}},height=10,width=2,depth=10,zlabel=256}};
        \pic[shift={(0.5,0,0)}] at (deconv2-east) {Conv={blockname=final,
            numFilters={{"3",""}},height=10,width=0.5,depth=10,zlabel=256}};
    \end{tikzpicture}
    \caption{Generator architecture from left to right. The centre layer is the residual layer.}
    \label{fig:generatorarch}
\end{figure}

For the PyTorch model, the implementation from the original authors of the paper was used. We then implemented this model as faithfully as possible in TensorFlow. Both models were trained on two different \hyperref[sec:datasets]{datasets}.
% a set of photographs of Yosemite National Park, CA, US taken in both Summer and Winter, and a set of photographs of flowers taken with an iPhone and a DSLR camera. There are 1812 images in the Summer training set and 3325 images in the Winter training set, although a batch size of 200 was used.

The \textit{generator} architecture, shown in figure \ref{fig:generatorarch}, is made up of three convolution layers, an inner ResNet\cite{resnet} and three deconvolution layers. The convolution and deconvolution layers have ReLU activations. The inner ResNet is made up of nine residual blocks, which sum the input to the block with the two convolutions of it, and help the network `retain' information.

At a high level, the idea is that the downscaling convolution layers extract the features of the image, the ResNet applies the transformation $A \rightarrow B$ to this information, and the upscaling deconvolution layers then generate the image from the transformed information. 

\subsection{PyTorch}
There are currently no CycleGAN implementations built into the PyTorch framework.
The original implementation accompanying the paper was in \href{https://github.com/junyanz/CycleGAN}{Torch}, however the authors also provided an implementation for \href{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}{PyTorch}, which is the one used for this paper, with default arguments and hyperparameters. 

\begin{figure}
    \centering
    \begin{tikzpicture}
        \tikzstyle{connection}=[ultra thick,every node/.style={sloped,allow upside down},draw=\edgecolor,opacity=0.7]

        \pic at (0,0,0) {ConvRelu={blockname=conv1,
            numFilters={{"3",""}},height=10,width={0.5},depth=10,zlabel=256}};
        \pic[shift={(0.5,0,0)}] at (conv1-east) {ConvRelu={blockname=conv2,
            numFilters={{"64",""}},height=10,width=2,depth=10,zlabel=256}};
        \pic[shift={(0.5,0,0)}] at (conv2-east) {ConvRelu={blockname=conv3,
            numFilters={{"128",""}},height=5,width=4,depth=5,zlabel=128}};
        \pic[shift={(0.5,0,0)}] at (conv3-east) {ConvRelu={blockname=conv4,
            numFilters={{"128",""}},height=5,width=4,depth=5,zlabel=128}};
        \pic[shift={(0.5,0,0)}] at (conv4-east) {Conv={blockname=conv5,
            numFilters={{"128",""}},height=5,width=4,depth=5,zlabel=128}};
    \end{tikzpicture}
    \caption{Discriminator architecture from left to right.}
    \label{fig:discriminatorarch}
\end{figure}

The \textit{discriminator} is a ConvNet that consists of 4 layers with Leaky ReLU activations and one layer without.

As per the definition of CycleGAN, two generators and two discriminators are used.
Each network is optimised with Adam \cite{adam_optimizer}, a common extension of Stochastic Gradient Descent, using \texttt{torch.optim.Adam}.

\subsection{TensorFlow}
TensorFlow provides a suite of tools for GANs in \texttt{tf.contrib.gan}. It has functions for creating, training and evaluating GANs, including both the original model as well as variants such as StarGAN\cite{stargan}, ACGAN\cite{acgan} and of course, CycleGAN.
For CycleGAN, TensorFlow provides \texttt{tf.contrib.gan.cyclegan\_model}. The user of the API must supply the generator and discriminator, and TensorFlow will handle setting up the model to connect the two together.

Generator and discriminator models were created using the high-level \texttt{tf.contrib.layers} counterparts to the PyTorch implementation, including batch normalisation and reflective padding. The functions in \texttt{tf.contrib.gan} were left to their default parameters where possible, which matched those of the original paper.

The images needed to be preprocessed so that they had pixel values in the range $[-1,1]$, and that they were all the same size. Additionally, there is reflective padding at the first, last and residual block convolution layers to prevent artefacts around image edges.
\section{Datasets}
\label{sec:datasets}
\begin{enumerate}
    \item \textbf{Yosemite National Park (Summer/Winter): } This dataset consists of 1273 Summer and 854 Winter pictures from the park. All images are originally from \textit{Flickr}, a social networking application where users upload images. The images were scaled to 256x256 pixels. \cite{GanTraining}
    \item \textbf{iPhone-DSLR camera images: } This dataset consists of 1813 iPhone and 3326 DSLR camera pictures. Once again these images were taken from \textit{Flickr} with the search tags \textit{fower} and using filters to search for images taken from iPhones and DSLR cameras with shallow depth of field.\cite{GanTraining}
\end{enumerate}
\section{Results and Discussion}

Both networks had satisfactory results. Results for converting winter to summer are shown in figure~\ref{fig:winter2summer}. TensorFlow emphasised making the vegetation appear greener/brighter much more so than PyTorch, and in some cases learnt to convert snow to lake water.

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        width=\columnwidth,
        unit vector ratio*=1 2,
        xmin=0,
        xmax=10,
        ymin=0,
        ymax=3,
        xtick={1,3,5,7,9},
        xticklabels={2,4,6,8,10},
        ytick={0.5,1.5,2.5},
        yticklabels={PyTorch, TensorFlow, Input},
        xlabel=Epoch
        ]
        
        % Original images
        \addplot graphics[xmin=0,ymin=2,xmax=2,ymax=3] {progressImages/real_epoch002};
        \addplot graphics[xmin=2,ymin=2,xmax=4,ymax=3] {progressImages/real_epoch004};
        \addplot graphics[xmin=4,ymin=2,xmax=6,ymax=3] {progressImages/real_epoch006};
        \addplot graphics[xmin=6,ymin=2,xmax=8,ymax=3] {progressImages/real_epoch008};
        \addplot graphics[xmin=8,ymin=2,xmax=10,ymax=3] {progressImages/real_epoch010};
        
        % TensorFlow images
        \addplot graphics[xmin=0,ymin=1,xmax=2,ymax=2] {progressImages/tensorflow_epoch002};
        \addplot graphics[xmin=2,ymin=1,xmax=4,ymax=2] {progressImages/tensorflow_epoch004};
        \addplot graphics[xmin=4,ymin=1,xmax=6,ymax=2] {progressImages/tensorflow_epoch006};
        \addplot graphics[xmin=6,ymin=1,xmax=8,ymax=2] {progressImages/tensorflow_epoch008};
        \addplot graphics[xmin=8,ymin=1,xmax=10,ymax=2] {progressImages/tensorflow_epoch010};

        % PyTorch images
        \addplot graphics[xmin=0,ymin=0,xmax=2,ymax=1] {progressImages/pytorch_epoch002};
        \addplot graphics[xmin=2,ymin=0,xmax=4,ymax=1] {progressImages/pytorch_epoch004};
        \addplot graphics[xmin=4,ymin=0,xmax=6,ymax=1] {progressImages/pytorch_epoch006};
        \addplot graphics[xmin=6,ymin=0,xmax=8,ymax=1] {progressImages/pytorch_epoch008};
        \addplot graphics[xmin=8,ymin=0,xmax=10,ymax=1] {progressImages/pytorch_epoch010};
    \end{axis}
    \end{tikzpicture}
    
    \caption{Progress made during training}
    
    \label{fig:trainingprogress}
\end{figure}

Figure~\ref{fig:cycleconsistencyloss} shows the cycle-consistency loss (CCL) between the two different frameworks. At first glance PyTorch has a much larger loss, but when viewing the results qualitatively there was very little difference. The reason for the discrepancy in the CCLs is that TensorFlow\footnote{See the  \href{https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/contrib/gan/python/losses/python/losses_impl.py?fbclid=IwAR2-OjbG_uheJzR1d93U4T6mH6AIBHx6xvkrT5nTYdbuJkfVUFdtPxGaF0E#L954-L955}{source} 
for more information} normalises the losses so that it is always between 1 and 0. This ensures that the CCL weight does not depend on the size of the images.

However both of the generator and discriminator losses for TensorFlow and PyTorch, shown in figure~\ref{fig:gendisclosses}, are extremely noisy, but this is expected due to the competitive nature of GANs\cite{qi2017loss}. It behaves similar to a zero-sum game where a decrease in the generator loss correlates to an increase in the discriminator loss, and vice-versa.
Figure~\ref{fig:trainingprogress} shows that qualitatively, the PyTorch implementation appeared to converge much faster. We are unsure as to why this is the case.


\begin{figure}
    \centering
    \includegraphics[width=0.3\columnwidth]{testImages/test_image_4_winter.jpg}
    \includegraphics[width=0.3\columnwidth]{testImages/tensorflow/test_image_4_w2s.jpg}
    \includegraphics[width=0.3\columnwidth]{testImages/pytorch/test_image_4_w2s.png} \\ 
    \includegraphics[width=0.3\columnwidth]{testImages/winter0.jpg}
    \includegraphics[width=0.3\columnwidth]{testImages/tensorflow/winter0_w2s.jpg}
    \includegraphics[width=0.3\columnwidth]{testImages/pytorch/winter0_w2s.png}
    \caption{From left to right: Original winter, TensorFlow summer, PyTorch summer}
    \label{fig:winter2summer}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.3\columnwidth]{testImages/summer1.jpg}
    \includegraphics[width=0.3\columnwidth]{testImages/tensorflow/summer1_s2w.jpg}
    \includegraphics[width=0.3\columnwidth]{testImages/tensorflow/summer1_s2w.jpg} \\ 
    \includegraphics[width=0.3\columnwidth]{testImages/test_image_2_summer.jpg}
    \includegraphics[width=0.3\columnwidth]{testImages/tensorflow/test_image_2_s2w.jpg}
    \includegraphics[width=0.3\columnwidth]{testImages/pytorch/test_image_2_s2w.png}
    \caption{From left to right: Original summer, TensorFlow winter, PyTorch winter}
    \label{fig:summer2winter}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{axis} [
        title=Cycle Consistency Loss,
        xlabel=Epochs,
        ylabel=Loss,
        no markers,
        cycle multi list={
            orange,purple\nextlist
            dashed,solid
        }
    ]
        \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/summer2winter_yosemite/cycleConsistencyLoss.csv};
        \addlegendentry{TensorFlow $\text{Summer} \rightarrow \text{Winter}$}
        \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/iphone2dslr_flower/cycleConsistencyLoss.csv};
        \addlegendentry{TensorFlow $\text{iPhone} \rightarrow \text{DSLR}$}
        
        \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={100}]{csvs/pytorch/summer2winter_yosemite/cycleConsistencyLoss.csv};
        \addlegendentry{PyTorch $\text{Summer} \rightarrow \text{Winter}$}
        \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={100}]{csvs/pytorch/iphone2dslr_flower/cycleConsistencyLoss.csv};
        \addlegendentry{PyTorch $\text{iPhone} \rightarrow \text{DSLR}$}
    \end{axis}
    \end{tikzpicture}
    \caption{Cycle-Consistency Loss (Average of both generators). (PyTorch $\text{iPhone} \rightarrow \text{DSLR}$ was only trained for 60 epochs}
    \label{fig:cycleconsistencyloss}
\end{figure}

\section{Limitations}
The model fails on images with snow in the immediate landscape, as it not always able to completely remove it. It also has difficulty converting images of landscapes outside of Yosemite and mountain ranges.

We observed small, checkerboard artefacts in the output images. Odena et al.\cite{odena2016deconvolution} describe the cause of these and methods to circumvent it.

\section{Further Work}
\subsection{Converting to DSLR}
\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{dslr2iphone.png}
        \caption{DSLR to phone, back to DSLR.}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{iphone2dslr.png}
        \caption{Phone to DSLR, back to phone.}
    \end{subfigure}
    \caption{Images converted with CycleGAN to look like they were taken with either a phone or a DSLR. Bokeh gets added in the generated DSLR images, whilst it is stripped in the phone pictures.}
    \label{fig:dslr2iphone}
\end{figure}

The TensorFlow model was also trained on a dataset of images of flowers, taken on phones and DSLRs. This dataset is also taken from the CycleGAN repository, and took around 20 hours to train. The results are shown in figure~\ref{fig:dslr2iphone}. The model learnt that DSLR photos typically have a bokeh effect, but unfortunately it also tries to swap out the colours. This may have been caused by one dataset having a particular set of colours more-so than the other, and we speculate this might be fixed by having a larger dataset to prevent overfitting. Interestingly enough though, the cycle-consistency loss also teaches the network how to reverse the colour swap almost perfectly.

\subsection{Face-off facial recreation in Keras}
This CycleGAN attempts to create a new face from an input face. Results are displayed in video format. The generated face mimics the facial expression of the input face. This architecture is implemented in Keras, a popular high level framework for machine learning. For the Generator a UNet\cite{unet} is used. For the discriminator the network is a convolutional network with Leaky RELU as well as batch normalisation. An Adam\cite{adam_optimizer} optimiser is employed in the training process. 
\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{cyclegan_face.png}
        \caption{From top to bottom: Input, Fake, Recreate of the input.}
    \end{subfigure}
    \caption{Face-off facial recreation}
\end{figure}

% Some macros for the datasets
\def\STW{$\text{Summer} \rightarrow \text{Winter}$}
\def\WTS{$\text{Winter} \rightarrow \text{Summer}$}
\def\iTD{$\text{iPhone} \rightarrow \text{DSLR}$}
\def\DTi{$\text{DSLR} \rightarrow \text{iPhone}$}
    
\begin{figure*}
    \centering
    \begin{subfigure}{0.5\linewidth}
        \begin{tikzpicture}
        \begin{axis}[
            xlabel=Epochs,
            ylabel=Loss (logarithmic),
            ymode=log,
            no markers,
            cycle multi list={
                orange,purple\nextlist
                dashed,solid
            }
        ]
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/summer2winter_yosemite/S2WDiscLoss.csv};
            \addlegendentry{TensorFlow \STW}
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/iphone2dslr_flower/i2DDiscLoss.csv};
            \addlegendentry{TensorFlow \iTD}

            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={150}]{csvs/pytorch/summer2winter_yosemite/S2WDiscLoss.csv};
            \addlegendentry{PyTorch \STW}
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={150}]{csvs/pytorch/iphone2dslr_flower/i2DDiscLoss.csv};
            \addlegendentry{PyTorch \iTD}
        \end{axis}
        \end{tikzpicture}
        \caption{Discriminator loss $\text{X} \rightarrow \text{Y}$}
    \end{subfigure}
    ~
    \begin{subfigure}{0.5\linewidth}
        \begin{tikzpicture}
        \begin{axis}[
            xlabel=Epochs,
            ylabel=Loss (logarithmic),
            ymode=log,
            no markers,
            cycle multi list={
                orange,purple\nextlist
                dashed,solid
            }
        ]
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/summer2winter_yosemite/W2SDiscLoss.csv};
            \addlegendentry{TensorFlow \WTS}
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/iphone2dslr_flower/D2iDiscLoss.csv};
            \addlegendentry{TensorFlow \DTi}

            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={150}]{csvs/pytorch/summer2winter_yosemite/W2SDiscLoss.csv};
            \addlegendentry{PyTorch \WTS}
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={150}]{csvs/pytorch/iphone2dslr_flower/D2iDiscLoss.csv};
            \addlegendentry{PyTorch \DTi}
        \end{axis}
        \end{tikzpicture}
        \caption{Discriminator loss $\text{Y} \rightarrow \text{X}$}
    \end{subfigure}
    \\
    \begin{subfigure}{0.5\linewidth}
        \begin{tikzpicture}
        \begin{axis}[
            xlabel=Epochs,
            ylabel=Loss (logarithmic),
            ymode=log,
            no markers,
            cycle multi list={
                orange,purple\nextlist
                dashed,solid
            },
            legend pos=south east
        ]
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/summer2winter_yosemite/S2WGenLoss.csv};
            \addlegendentry{TensorFlow \STW}
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/iphone2dslr_flower/i2DGenLoss.csv};
            \addlegendentry{TensorFlow \iTD}

            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={150}]{csvs/pytorch/summer2winter_yosemite/S2WGenLoss.csv};
            \addlegendentry{PyTorch \STW}
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={150}]{csvs/pytorch/iphone2dslr_flower/i2DGenLoss.csv};
            \addlegendentry{PyTorch \iTD}
        \end{axis}
        \end{tikzpicture}
        \caption{Generator loss $\text{X} \rightarrow \text{Y}$}
    \end{subfigure}
    ~
    \begin{subfigure}{0.5\linewidth}
        \begin{tikzpicture}
        \begin{axis}[
            xlabel=Epochs,
            ylabel=Loss (logarithmic),
            ymode=log,
            no markers,
            cycle multi list={
                orange,purple\nextlist
                dashed,solid
            },
            legend pos=south east
        ]
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/summer2winter_yosemite/W2SGenLoss.csv};
            \addlegendentry{TensorFlow \WTS}
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={50}]{csvs/tensorflow/iphone2dslr_flower/D2iGenLoss.csv};
            \addlegendentry{TensorFlow \DTi}

            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={150}]{csvs/pytorch/summer2winter_yosemite/W2SGenLoss.csv};
            \addlegendentry{PyTorch \WTS}
            \addplot table [x=Epoch, y=Value, col sep=comma, each nth point={150}]{csvs/pytorch/iphone2dslr_flower/D2iGenLoss.csv};
            \addlegendentry{PyTorch \DTi}
        \end{axis}
        \end{tikzpicture}
        \caption{Generator loss $\text{Y} \rightarrow \text{S}$}
    \end{subfigure}
    
    \caption{Individual losses of the generators and discriminators}
    \label{fig:gendisclosses}
\end{figure*}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\end{document}


